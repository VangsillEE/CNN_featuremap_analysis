{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b62dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0242f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {\n",
    "    0: \"Airplane\",\n",
    "    1: \"Automobile\",\n",
    "    2: \"Bird\",\n",
    "    3: \"Cat\",\n",
    "    4: \"Deer\",\n",
    "    5: \"Dog\",\n",
    "    6: \"Frog\",\n",
    "    7: \"Horse\",\n",
    "    8: \"Ship\",\n",
    "    9: \"Truck\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b15dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_1i = torch.load('model_1_initial.pth')\n",
    "weights_15 = torch.load('model_1_epoch_5.pth')\n",
    "weights_110 = torch.load('model_1_epoch_10.pth')\n",
    "weights_2i = torch.load('model_2_initial.pth')\n",
    "weights_25 = torch.load('model_2_epoch_5.pth')\n",
    "weights_210 = torch.load('model_2_epoch_10.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ce45eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['features.0.weight', 'features.0.bias', 'features.2.weight', 'features.2.bias', 'features.5.weight', 'features.5.bias', 'features.7.weight', 'features.7.bias', 'features.10.weight', 'features.10.bias', 'features.12.weight', 'features.12.bias', 'features.14.weight', 'features.14.bias', 'features.17.weight', 'features.17.bias', 'features.19.weight', 'features.19.bias', 'features.21.weight', 'features.21.bias', 'features.24.weight', 'features.24.bias', 'features.26.weight', 'features.26.bias', 'features.28.weight', 'features.28.bias', 'classifier.0.weight', 'classifier.0.bias', 'classifier.3.weight', 'classifier.3.bias', 'classifier.6.weight', 'classifier.6.bias'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "52bfb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 이미지 크기를 VGGNet의 입력 크기로 변경\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f924c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7d883b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 5, 6, 5, 4, 1, 1, 4, 6, 6, 7, 1, 8, 1, 3, 1, 0, 8, 2, 3, 7, 5, 2,\n",
      "        1, 5, 6, 3, 8, 7, 5, 1, 5, 2, 3, 4, 4, 7, 9, 3, 3, 3, 6, 0, 3, 9, 1, 8,\n",
      "        9, 1, 1, 1, 6, 9, 3, 8, 0, 1, 0, 7, 8, 2, 0, 9])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5cbd3c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 5, 6, 5, 4, 1, 1, 4, 6, 6, 7, 1, 8, 1, 3, 1, 0, 8, 2, 3, 7, 5, 2,\n",
      "        1, 5, 6, 3, 8, 7, 5, 1, 5, 2, 3, 4, 4, 7, 9, 3, 3, 3, 6, 0, 3, 9, 1, 8,\n",
      "        9, 1, 1, 1, 6, 9, 3, 8, 0, 1, 0, 7, 8, 2, 0, 9])\n"
     ]
    }
   ],
   "source": [
    "image = images[22]\n",
    "label = labels[22]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "56bd32b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "251bcaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c412c9a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 이미지를 넘파이 배열로 변환 (텐서를 넘파이로 변환하여 시각화에 사용)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m image_np \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 이미지 시각화\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# 이미지를 넘파이 배열로 변환 (텐서를 넘파이로 변환하여 시각화에 사용)\n",
    "image_np = image.numpy()\n",
    "\n",
    "# 이미지 시각화\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(np.transpose(image_np, (1, 2, 0)))\n",
    "plt.title(class_labels[label.item()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e0a48806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16 = models.vgg16(pretrained=False)\n",
    "vgg16.classifier[6] = nn.Linear(4096, 10)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg16.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "08e6dc85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.load_state_dict(weights_110)\n",
    "vgg16.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "56d50df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Size = torch.Size([64, 224, 224]), Channels = 224\n",
      "Layer 2: Size = torch.Size([64, 224, 224]), Channels = 224\n",
      "Layer 3: Size = torch.Size([128, 112, 112]), Channels = 112\n",
      "Layer 4: Size = torch.Size([128, 112, 112]), Channels = 112\n",
      "Layer 5: Size = torch.Size([256, 56, 56]), Channels = 56\n",
      "Layer 6: Size = torch.Size([256, 56, 56]), Channels = 56\n",
      "Layer 7: Size = torch.Size([256, 56, 56]), Channels = 56\n",
      "Layer 8: Size = torch.Size([512, 28, 28]), Channels = 28\n",
      "Layer 9: Size = torch.Size([512, 28, 28]), Channels = 28\n",
      "Layer 10: Size = torch.Size([512, 28, 28]), Channels = 28\n",
      "Layer 11: Size = torch.Size([512, 14, 14]), Channels = 14\n",
      "Layer 12: Size = torch.Size([512, 14, 14]), Channels = 14\n",
      "Layer 13: Size = torch.Size([512, 14, 14]), Channels = 14\n"
     ]
    }
   ],
   "source": [
    "# 이미지를 VGG-16 합성곱 레이어들을 통과시키기\n",
    "conv_feature_maps = []\n",
    "for layer in vgg16.features:\n",
    "    image = layer(image)\n",
    "    if isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "        conv_feature_maps.append(image)\n",
    "        \n",
    "# 각 레이어의 출력 크기와 채널 수 출력\n",
    "for i, feature_map in enumerate(conv_feature_maps):\n",
    "    print(f\"Layer {i+1}: Size = {feature_map.size()}, Channels = {feature_map.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b63af735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 14, 14])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_feature_maps[12].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8bf40524",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3636669363.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[156], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    conv_feature_maps[12][0].cpu().detach().numpy().transpose((1, 0)\u001b[0m\n\u001b[1;37m                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "conv_feature_maps[12][0].cpu().detach().numpy().transpose((1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "251dcca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d55eab1810>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaLElEQVR4nO3df3CUhb3v8c+SkCVwk6WJk4Q9JBjm5gxIECnBDj+UMGrOREQ9jlpEhSvtGRkjEJmxQJGKdMgW2zLMkAIT/qD0MEHuTBGoV1tTBSKDDCEBZWhHpGZIlObk6KW7IZQlP57zR497jImRJM/uNxver5n9I88+4fvd6XTffZLtE4/jOI4AADAwzHoBAMDNiwgBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzidYLfF1nZ6cuXbqklJQUeTwe63UAAH3kOI5aWlrk9/s1bFjv1zqDLkKXLl1Sdna29RoAgAFqbGzU2LFjez1n0EUoJSVFkjRb9ytRw423AaLDkxib/+o57e0xmSNJCelpMZnT8cX/j8kc9F+72nRMb0bez3sz6CL05Y/gEjVciR4ihKHJ44lRhGL4I+2EYUkxmePhfWHw++87kt7Ir1T4YAIAwAwRAgCYIUIAADNECABghggBAMwQIQCAmahFaNu2bcrNzdWIESM0bdo0vffee9EaBQCIU1GJ0L59+1RaWqq1a9fq9OnTuuuuu1RcXKyGhoZojAMAxKmoRGjz5s36wQ9+oB/+8IeaOHGitmzZouzsbG3fvj0a4wAAccr1CF2/fl21tbUqKirqcryoqEjHjx/vdn44HFYoFOryAADcHFyP0Oeff66Ojg5lZmZ2OZ6ZmammpqZu5wcCAfl8vsiDm5cCwM0jah9M+Po9gxzH6fE+QmvWrFEwGIw8Ghsbo7USAGCQcf0uirfccosSEhK6XfU0Nzd3uzqSJK/XK6/X6/YaAIA44PqVUFJSkqZNm6aqqqoux6uqqjRz5ky3xwEA4lhU7ie/cuVKPf300yooKNCMGTNUUVGhhoYGLV26NBrjAABxKioR+v73v68vvvhCGzZs0F//+lfl5+frzTff1Lhx46IxDgAQpzyO4zjWS3xVKBSSz+dToR7ij9phyBqSf1n1lvSYzOn4/IuYzEH/tTttOqKDCgaDSk1N7fVc7h0HADBDhAAAZogQAMAMEQIAmCFCAAAzsfmIDhAHErPHxmxW/aKcmMzJ3tj9psHRwqfW0B9cCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAICZROsFgG8zbMSImMz584v/FJM5kjRxy6WYzGl5+M6YzJGk5AMnYzYLQwdXQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADOuRygQCGj69OlKSUlRRkaGHn74YX300UdujwEADAGuR+jo0aMqKSnRiRMnVFVVpfb2dhUVFam1tdXtUQCAOOf6veN+//vfd/l6165dysjIUG1tre6++263xwEA4ljUb2AaDAYlSWlpaT0+Hw6HFQ6HI1+HQqForwQAGCSi+sEEx3G0cuVKzZ49W/n5+T2eEwgE5PP5Io/s7OxorgQAGESiGqHnn39eH374ofbu3fuN56xZs0bBYDDyaGxsjOZKAIBBJGo/jlu2bJkOHTqk6upqjR079hvP83q98nq90VoDADCIuR4hx3G0bNkyvf766zpy5Ihyc3PdHgEAGCJcj1BJSYkqKyt18OBBpaSkqKmpSZLk8/mUnJzs9jgAQBxz/XdC27dvVzAYVGFhocaMGRN57Nu3z+1RAIA4F5UfxwEAcCO4dxwAwAwRAgCYIUIAADNECABghggBAMxE/QamwEBd2DA1JnPGvNcZkzmS9OmD/pjM+eBH22IyR5L+5cAdMZsVK397ekZM5txS9UlM5rQ3/UdM5vQFV0IAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABgJtF6AcSvxLH/FJM57be0xWTOqM9iM0eS/uNfPTGZk/v//i0mcyTpn1UTs1mxMvrf34/JnPaYTBmcuBICAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwEzUIxQIBOTxeFRaWhrtUQCAOBPVCNXU1KiiokK33357NMcAAOJU1CJ05coVPfnkk9q5c6e+853vRGsMACCORS1CJSUlmjdvnu69995ezwuHwwqFQl0eAICbQ1RuYPraa6+prq5ONTXffkPDQCCgV155JRprAAAGOdevhBobG7VixQrt2bNHI0aM+Nbz16xZo2AwGHk0Nja6vRIAYJBy/UqotrZWzc3NmjZtWuRYR0eHqqurVV5ernA4rISEhMhzXq9XXq/X7TUAAHHA9Qjdc889Onv2bJdjzzzzjCZMmKBVq1Z1CRAA4ObmeoRSUlKUn5/f5dioUaOUnp7e7TgA4ObGHRMAAGZi8ue9jxw5EosxAIA4w5UQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgJmYfEQbQ9Onj46LyZzUs99+jhuGX2qOzSBJeUtiM6vz2rWYzAH6iyshAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMJNovQDcdfn/zIjZrIz5jTGZM+ye2Mxpj8kUAF/FlRAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZqISoc8++0xPPfWU0tPTNXLkSN1xxx2qra2NxigAQBxz/Y4Jly9f1qxZszR37ly99dZbysjI0F/+8heNHj3a7VEAgDjneoQ2bdqk7Oxs7dq1K3Ls1ltvdXsMAGAIcP3HcYcOHVJBQYEee+wxZWRkaOrUqdq5c+c3nh8OhxUKhbo8AAA3B9cj9Mknn2j79u3Ky8vTH/7wBy1dulTLly/Xb37zmx7PDwQC8vl8kUd2drbbKwEABimP4ziOm/9gUlKSCgoKdPz48cix5cuXq6amRu+//36388PhsMLhcOTrUCik7OxsFeohJXqGu7naTSGWd9H2Pf1pTObE6i7aANzR7rTpiA4qGAwqNTW113NdvxIaM2aMbrvtti7HJk6cqIaGhh7P93q9Sk1N7fIAANwcXI/QrFmz9NFHH3U5dv78eY0bN87tUQCAOOd6hF544QWdOHFCZWVlunDhgiorK1VRUaGSkhK3RwEA4pzrEZo+fbpef/117d27V/n5+frpT3+qLVu26Mknn3R7FAAgzkXlz3s/8MADeuCBB6LxTwMAhhDuHQcAMEOEAABmiBAAwAwRAgCYIUIAADNECABgJiof0XZD4li/Eod5ozrjbzPGRvXf/6ril47EZE7d3y7EZI4kNfz7/47JnHRx7zhgqOJKCABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABgJtF6gW/S/uklyTM8qjP+V+OnUf33v+q9/zsiRpP+M0ZzpPQYzsLglzDaF5M5Tlt7TOZIUmdra8xm3ay4EgIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJhxPULt7e166aWXlJubq+TkZI0fP14bNmxQZ2en26MAAHHO9dv2bNq0STt27NDu3bs1adIknTp1Ss8884x8Pp9WrFjh9jgAQBxzPULvv/++HnroIc2bN0+SdOutt2rv3r06deqU26MAAHHO9R/HzZ49W++8847Onz8vSfrggw907Ngx3X///T2eHw6HFQqFujwAADcH16+EVq1apWAwqAkTJighIUEdHR3auHGjnnjiiR7PDwQCeuWVV9xeAwAQB1y/Etq3b5/27NmjyspK1dXVaffu3frFL36h3bt393j+mjVrFAwGI4/Gxka3VwIADFKuXwm9+OKLWr16tRYsWCBJmjx5si5evKhAIKDFixd3O9/r9crr9bq9BgAgDrh+JXT16lUNG9b1n01ISOAj2gCAbly/Epo/f742btyonJwcTZo0SadPn9bmzZu1ZMkSt0cBAOKc6xHaunWr1q1bp+eee07Nzc3y+/169tln9ZOf/MTtUQCAOOdxHMexXuKrQqGQfD6fCvWQEj3DrdcBcIMSRvtiMsdpa4/JHEnqbG2N2ayhpN1p0xEdVDAYVGpqaq/ncu84AIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADOu//+EANycOv4WtF4BcYgrIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGb6HKHq6mrNnz9ffr9fHo9HBw4c6PK84zhav369/H6/kpOTVVhYqHPnzrm1LwBgCOlzhFpbWzVlyhSVl5f3+Pyrr76qzZs3q7y8XDU1NcrKytJ9992nlpaWAS8LABhaEvv6DcXFxSouLu7xOcdxtGXLFq1du1aPPPKIJGn37t3KzMxUZWWlnn322YFtCwAYUlz9nVB9fb2amppUVFQUOeb1ejVnzhwdP368x+8Jh8MKhUJdHgCAm4OrEWpqapIkZWZmdjmemZkZee7rAoGAfD5f5JGdne3mSgCAQSwqn47zeDxdvnYcp9uxL61Zs0bBYDDyaGxsjMZKAIBBqM+/E+pNVlaWpH9cEY0ZMyZyvLm5udvV0Ze8Xq+8Xq+bawAA4oSrV0K5ubnKyspSVVVV5Nj169d19OhRzZw5081RAIAhoM9XQleuXNGFCxciX9fX1+vMmTNKS0tTTk6OSktLVVZWpry8POXl5amsrEwjR47UwoULXV0cABD/+hyhU6dOae7cuZGvV65cKUlavHixfv3rX+tHP/qR/v73v+u5557T5cuX9b3vfU9vv/22UlJS3NsaADAkeBzHcayX+KpQKCSfz6dCPaREz3DrdQAAfdTutOmIDioYDCo1NbXXc7l3HADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmOlzhKqrqzV//nz5/X55PB4dOHAg8lxbW5tWrVqlyZMna9SoUfL7/Vq0aJEuXbrk5s4AgCGizxFqbW3VlClTVF5e3u25q1evqq6uTuvWrVNdXZ3279+v8+fP68EHH3RlWQDA0JLY128oLi5WcXFxj8/5fD5VVVV1ObZ161bdeeedamhoUE5OTv+2BAAMSX2OUF8Fg0F5PB6NHj26x+fD4bDC4XDk61AoFO2VAACDRFQ/mHDt2jWtXr1aCxcuVGpqao/nBAIB+Xy+yCM7OzuaKwEABpGoRaitrU0LFixQZ2entm3b9o3nrVmzRsFgMPJobGyM1koAgEEmKj+Oa2tr0+OPP676+nq9++6733gVJEler1derzcaawAABjnXI/RlgD7++GMdPnxY6enpbo8AAAwRfY7QlStXdOHChcjX9fX1OnPmjNLS0uT3+/Xoo4+qrq5Ob7zxhjo6OtTU1CRJSktLU1JSknubAwDinsdxHKcv33DkyBHNnTu32/HFixdr/fr1ys3N7fH7Dh8+rMLCwm/990OhkHw+nwr1kBI9w/uyGgBgEGh32nREBxUMBnv9dYzUjyuhwsJC9datPjYNAHAT495xAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADATKL1Al/nOI4kqV1tkmO8DACgz9rVJul/3s97M+gi1NLSIkk6pjeNNwEADERLS4t8Pl+v53icG0lVDHV2durSpUtKSUmRx+O54e8LhULKzs5WY2OjUlNTo7hhbAy11yPxmuIFr2nwG+yvx3EctbS0yO/3a9iw3n/rM+iuhIYNG6axY8f2+/tTU1MH5X8o/TXUXo/Ea4oXvKbBbzC/nm+7AvoSH0wAAJghQgAAM0MmQl6vVy+//LK8Xq/1Kq4Yaq9H4jXFC17T4DeUXs+g+2ACAODmMWSuhAAA8YcIAQDMECEAgBkiBAAwMyQitG3bNuXm5mrEiBGaNm2a3nvvPeuV+i0QCGj69OlKSUlRRkaGHn74YX300UfWa7kqEAjI4/GotLTUepUB+eyzz/TUU08pPT1dI0eO1B133KHa2lrrtfqlvb1dL730knJzc5WcnKzx48drw4YN6uzstF7thlVXV2v+/Pny+/3yeDw6cOBAl+cdx9H69evl9/uVnJyswsJCnTt3zmbZG9Tba2pra9OqVas0efJkjRo1Sn6/X4sWLdKlS5fsFu6HuI/Qvn37VFpaqrVr1+r06dO66667VFxcrIaGBuvV+uXo0aMqKSnRiRMnVFVVpfb2dhUVFam1tdV6NVfU1NSooqJCt99+u/UqA3L58mXNmjVLw4cP11tvvaU//elP+uUvf6nRo0dbr9YvmzZt0o4dO1ReXq4///nPevXVV/Xzn/9cW7dutV7thrW2tmrKlCkqLy/v8flXX31VmzdvVnl5uWpqapSVlaX77rsvcr/Kwai313T16lXV1dVp3bp1qqur0/79+3X+/Hk9+OCDBpsOgBPn7rzzTmfp0qVdjk2YMMFZvXq10Ubuam5udiQ5R48etV5lwFpaWpy8vDynqqrKmTNnjrNixQrrlfpt1apVzuzZs63XcM28efOcJUuWdDn2yCOPOE899ZTRRgMjyXn99dcjX3d2djpZWVnOz372s8ixa9euOT6fz9mxY4fBhn339dfUk5MnTzqSnIsXL8ZmKRfE9ZXQ9evXVVtbq6Kioi7Hi4qKdPz4caOt3BUMBiVJaWlpxpsMXElJiebNm6d7773XepUBO3TokAoKCvTYY48pIyNDU6dO1c6dO63X6rfZs2frnXfe0fnz5yVJH3zwgY4dO6b777/feDN31NfXq6mpqct7hdfr1Zw5c4bMe4X0j/cLj8cTV1fkg+4Gpn3x+eefq6OjQ5mZmV2OZ2ZmqqmpyWgr9ziOo5UrV2r27NnKz8+3XmdAXnvtNdXV1ammpsZ6FVd88skn2r59u1auXKkf//jHOnnypJYvXy6v16tFixZZr9dnq1atUjAY1IQJE5SQkKCOjg5t3LhRTzzxhPVqrvjy/aCn94qLFy9arOS6a9euafXq1Vq4cOGgvalpT+I6Ql/6+p98cBynT38GYrB6/vnn9eGHH+rYsWPWqwxIY2OjVqxYobffflsjRoywXscVnZ2dKigoUFlZmSRp6tSpOnfunLZv3x6XEdq3b5/27NmjyspKTZo0SWfOnFFpaan8fr8WL15svZ5rhup7RVtbmxYsWKDOzk5t27bNep0+iesI3XLLLUpISOh21dPc3Nztf/HEm2XLlunQoUOqrq4e0J+2GAxqa2vV3NysadOmRY51dHSourpa5eXlCofDSkhIMNyw78aMGaPbbruty7GJEyfqt7/9rdFGA/Piiy9q9erVWrBggSRp8uTJunjxogKBwJCIUFZWlqR/XBGNGTMmcnwovFe0tbXp8ccfV319vd599924ugqS4vzTcUlJSZo2bZqqqqq6HK+qqtLMmTONthoYx3H0/PPPa//+/Xr33XeVm5trvdKA3XPPPTp79qzOnDkTeRQUFOjJJ5/UmTNn4i5AkjRr1qxuH50/f/68xo0bZ7TRwFy9erXbHx9LSEiIq49o9yY3N1dZWVld3iuuX7+uo0ePxu17hfQ/Afr444/1xz/+Uenp6dYr9VlcXwlJ0sqVK/X000+roKBAM2bMUEVFhRoaGrR06VLr1fqlpKRElZWVOnjwoFJSUiJXeT6fT8nJycbb9U9KSkq332mNGjVK6enpcfu7rhdeeEEzZ85UWVmZHn/8cZ08eVIVFRWqqKiwXq1f5s+fr40bNyonJ0eTJk3S6dOntXnzZi1ZssR6tRt25coVXbhwIfJ1fX29zpw5o7S0NOXk5Ki0tFRlZWXKy8tTXl6eysrKNHLkSC1cuNBw69719pr8fr8effRR1dXV6Y033lBHR0fk/SItLU1JSUlWa/eN7Yfz3PGrX/3KGTdunJOUlOR897vfjeuPM0vq8bFr1y7r1VwV7x/RdhzH+d3vfufk5+c7Xq/XmTBhglNRUWG9Ur+FQiFnxYoVTk5OjjNixAhn/Pjxztq1a51wOGy92g07fPhwj//dWbx4seM4//iY9ssvv+xkZWU5Xq/Xufvuu52zZ8/aLv0tentN9fX13/h+cfjwYevVbxh/ygEAYCaufycEAIhvRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAICZ/wKQpJfUPmNGywAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(conv_feature_maps[12][6].cpu().detach().numpy().transpose((1, 0)), cmap='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
